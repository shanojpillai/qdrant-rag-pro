{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QdrantRAG-Pro Analysis Notebook\n",
    "\n",
    "This notebook provides comprehensive analysis and experimentation capabilities for the QdrantRAG-Pro system.\n",
    "\n",
    "## Features\n",
    "- System performance analysis\n",
    "- Search quality evaluation\n",
    "- Embedding visualization\n",
    "- Response quality assessment\n",
    "- Comparative analysis of different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import QdrantRAG-Pro components\n",
    "from core.config.settings import Settings\n",
    "from core.database.qdrant_client import QdrantManager\n",
    "from core.database.document_store import DocumentStore\n",
    "from core.services.embedding_service import EmbeddingService\n",
    "from core.services.search_engine import HybridSearchEngine\n",
    "from core.services.response_generator import ResponseGenerator\n",
    "from core.models.document import Document, DocumentMetadata\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system components\n",
    "settings = Settings()\n",
    "qdrant_manager = QdrantManager(settings)\n",
    "embedding_service = EmbeddingService(settings)\n",
    "document_store = DocumentStore(qdrant_manager, settings)\n",
    "search_engine = HybridSearchEngine(qdrant_manager, embedding_service, settings)\n",
    "response_generator = ResponseGenerator(settings)\n",
    "\n",
    "print(\"‚úÖ System components initialized\")\n",
    "print(f\"üìä Collection: {settings.qdrant_collection_name}\")\n",
    "print(f\"ü§ñ Model: {settings.embedding_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system health\n",
    "health_status = qdrant_manager.health_check()\n",
    "collection_info = qdrant_manager.get_collection_info()\n",
    "document_count = document_store.get_document_count()\n",
    "cache_stats = embedding_service.get_cache_stats()\n",
    "\n",
    "print(f\"üîå Qdrant Health: {'‚úÖ Connected' if health_status else '‚ùå Disconnected'}\")\n",
    "if collection_info:\n",
    "    print(f\"üìö Total Vectors: {collection_info.vectors_count:,}\")\n",
    "    print(f\"üìÑ Main Documents: {document_count:,}\")\n",
    "    print(f\"üìê Vector Dimension: {collection_info.config.params.vectors.size}\")\n",
    "    print(f\"üìè Distance Metric: {collection_info.config.params.vectors.distance}\")\n",
    "else:\n",
    "    print(\"‚ùå Collection not found\")\n",
    "\n",
    "if cache_stats.get('cache_enabled', False):\n",
    "    print(f\"üíæ Cache: {cache_stats['size']}/{cache_stats['max_size']} entries\")\n",
    "else:\n",
    "    print(\"üíæ Cache: Disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries for analysis\n",
    "test_queries = [\n",
    "    \"vector database performance\",\n",
    "    \"How does RAG improve language models?\",\n",
    "    \"API v2.0 configuration\",\n",
    "    \"machine learning embeddings\",\n",
    "    \"What is hybrid search?\",\n",
    "    \"production deployment best practices\",\n",
    "    \"OpenAI embedding models\",\n",
    "    \"similarity search algorithms\",\n",
    "    \"information retrieval techniques\",\n",
    "    \"LLM integration strategies\"\n",
    "]\n",
    "\n",
    "print(f\"üìù Prepared {len(test_queries)} test queries for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform search performance analysis\n",
    "async def analyze_search_performance(queries: List[str], weight_configs: List[Dict[str, float]]):\n",
    "    \"\"\"Analyze search performance across different configurations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for config in weight_configs:\n",
    "        config_name = f\"V{config['vector']:.1f}_K{config['keyword']:.1f}\"\n",
    "        print(f\"Testing configuration: {config_name}\")\n",
    "        \n",
    "        for query in queries:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            search_results = await search_engine.search(\n",
    "                query=query,\n",
    "                limit=10,\n",
    "                vector_weight=config['vector'],\n",
    "                keyword_weight=config['keyword'],\n",
    "                auto_adjust_weights=False\n",
    "            )\n",
    "            \n",
    "            search_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_score = np.mean([r.combined_score for r in search_results]) if search_results else 0\n",
    "            max_score = max([r.combined_score for r in search_results]) if search_results else 0\n",
    "            result_count = len(search_results)\n",
    "            \n",
    "            results.append({\n",
    "                'config': config_name,\n",
    "                'vector_weight': config['vector'],\n",
    "                'keyword_weight': config['keyword'],\n",
    "                'query': query,\n",
    "                'search_time': search_time,\n",
    "                'result_count': result_count,\n",
    "                'avg_score': avg_score,\n",
    "                'max_score': max_score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define weight configurations to test\n",
    "weight_configs = [\n",
    "    {'vector': 1.0, 'keyword': 0.0},  # Pure vector search\n",
    "    {'vector': 0.8, 'keyword': 0.2},  # Vector-heavy\n",
    "    {'vector': 0.7, 'keyword': 0.3},  # Default\n",
    "    {'vector': 0.5, 'keyword': 0.5},  # Balanced\n",
    "    {'vector': 0.3, 'keyword': 0.7},  # Keyword-heavy\n",
    "    {'vector': 0.0, 'keyword': 1.0},  # Pure keyword search\n",
    "]\n",
    "\n",
    "# Run analysis\n",
    "performance_df = await analyze_search_performance(test_queries[:5], weight_configs)\n",
    "print(f\"‚úÖ Completed performance analysis with {len(performance_df)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize search performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Average search time by configuration\n",
    "time_by_config = performance_df.groupby('config')['search_time'].mean()\n",
    "axes[0, 0].bar(time_by_config.index, time_by_config.values)\n",
    "axes[0, 0].set_title('Average Search Time by Configuration')\n",
    "axes[0, 0].set_ylabel('Time (seconds)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average score by configuration\n",
    "score_by_config = performance_df.groupby('config')['avg_score'].mean()\n",
    "axes[0, 1].bar(score_by_config.index, score_by_config.values)\n",
    "axes[0, 1].set_title('Average Search Score by Configuration')\n",
    "axes[0, 1].set_ylabel('Average Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Result count by configuration\n",
    "count_by_config = performance_df.groupby('config')['result_count'].mean()\n",
    "axes[1, 0].bar(count_by_config.index, count_by_config.values)\n",
    "axes[1, 0].set_title('Average Result Count by Configuration')\n",
    "axes[1, 0].set_ylabel('Result Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Score distribution\n",
    "axes[1, 1].boxplot([performance_df[performance_df['config'] == config]['avg_score'].values \n",
    "                   for config in performance_df['config'].unique()],\n",
    "                  labels=performance_df['config'].unique())\n",
    "axes[1, 1].set_title('Score Distribution by Configuration')\n",
    "axes[1, 1].set_ylabel('Average Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nüìä Performance Summary:\")\n",
    "summary = performance_df.groupby('config').agg({\n",
    "    'search_time': ['mean', 'std'],\n",
    "    'avg_score': ['mean', 'std'],\n",
    "    'result_count': 'mean'\n",
    "}).round(4)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze response generation quality\n",
    "async def analyze_response_quality(questions: List[str]):\n",
    "    \"\"\"Analyze response generation quality.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Processing: {question[:50]}...\")\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        search_results = await search_engine.search(question, limit=5)\n",
    "        \n",
    "        if search_results:\n",
    "            # Generate response\n",
    "            response = await response_generator.generate_response(question, search_results)\n",
    "            \n",
    "            # Validate response quality\n",
    "            quality_metrics = response_generator.validate_response_quality(response)\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'confidence_score': response.confidence_score,\n",
    "                'confidence_level': response.confidence_level,\n",
    "                'source_coverage': response.source_coverage,\n",
    "                'sources_used': len(response.sources_used),\n",
    "                'processing_time': response.processing_time,\n",
    "                'answer_length': len(response.answer.split()),\n",
    "                'reasoning_steps': len(response.reasoning_steps),\n",
    "                'overall_quality': quality_metrics['overall_quality'],\n",
    "                'needs_review': response.needs_review,\n",
    "                'has_limitations': bool(response.limitations)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test questions for response quality analysis\n",
    "test_questions = [\n",
    "    \"What is Qdrant and how does it work?\",\n",
    "    \"How can I improve search performance in vector databases?\",\n",
    "    \"What are the benefits of hybrid search over pure vector search?\",\n",
    "    \"How do I deploy a RAG system in production?\",\n",
    "    \"What embedding model should I use for my application?\"\n",
    "]\n",
    "\n",
    "# Run response quality analysis\n",
    "response_df = await analyze_response_quality(test_questions)\n",
    "print(f\"\\n‚úÖ Completed response quality analysis for {len(response_df)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize response quality metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Confidence score distribution\n",
    "axes[0, 0].hist(response_df['confidence_score'], bins=10, alpha=0.7)\n",
    "axes[0, 0].set_title('Confidence Score Distribution')\n",
    "axes[0, 0].set_xlabel('Confidence Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Source coverage distribution\n",
    "axes[0, 1].hist(response_df['source_coverage'], bins=10, alpha=0.7)\n",
    "axes[0, 1].set_title('Source Coverage Distribution')\n",
    "axes[0, 1].set_xlabel('Source Coverage')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Processing time distribution\n",
    "axes[0, 2].hist(response_df['processing_time'], bins=10, alpha=0.7)\n",
    "axes[0, 2].set_title('Processing Time Distribution')\n",
    "axes[0, 2].set_xlabel('Processing Time (seconds)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# Answer length vs confidence\n",
    "axes[1, 0].scatter(response_df['answer_length'], response_df['confidence_score'])\n",
    "axes[1, 0].set_title('Answer Length vs Confidence')\n",
    "axes[1, 0].set_xlabel('Answer Length (words)')\n",
    "axes[1, 0].set_ylabel('Confidence Score')\n",
    "\n",
    "# Sources used vs confidence\n",
    "axes[1, 1].scatter(response_df['sources_used'], response_df['confidence_score'])\n",
    "axes[1, 1].set_title('Sources Used vs Confidence')\n",
    "axes[1, 1].set_xlabel('Number of Sources Used')\n",
    "axes[1, 1].set_ylabel('Confidence Score')\n",
    "\n",
    "# Quality distribution\n",
    "quality_counts = response_df['overall_quality'].value_counts()\n",
    "axes[1, 2].pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 2].set_title('Overall Quality Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display response quality summary\n",
    "print(\"\\nüìä Response Quality Summary:\")\n",
    "print(f\"Average Confidence: {response_df['confidence_score'].mean():.3f}\")\n",
    "print(f\"Average Source Coverage: {response_df['source_coverage'].mean():.3f}\")\n",
    "print(f\"Average Processing Time: {response_df['processing_time'].mean():.3f}s\")\n",
    "print(f\"Responses Needing Review: {response_df['needs_review'].sum()}/{len(response_df)}\")\n",
    "print(f\"Responses with Limitations: {response_df['has_limitations'].sum()}/{len(response_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding characteristics\n",
    "async def analyze_embeddings(sample_texts: List[str]):\n",
    "    \"\"\"Analyze embedding characteristics.\"\"\"\n",
    "    print(f\"Generating embeddings for {len(sample_texts)} texts...\")\n",
    "    \n",
    "    embedding_results = await embedding_service.create_embeddings_batch(sample_texts)\n",
    "    embeddings = np.array([result.embedding for result in embedding_results])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'dimension': embeddings.shape[1],\n",
    "        'mean_norm': np.mean(np.linalg.norm(embeddings, axis=1)),\n",
    "        'std_norm': np.std(np.linalg.norm(embeddings, axis=1)),\n",
    "        'mean_value': np.mean(embeddings),\n",
    "        'std_value': np.std(embeddings),\n",
    "        'min_value': np.min(embeddings),\n",
    "        'max_value': np.max(embeddings)\n",
    "    }\n",
    "    \n",
    "    return embeddings, stats, embedding_results\n",
    "\n",
    "# Sample texts for embedding analysis\n",
    "sample_texts = [\n",
    "    \"Vector databases enable efficient similarity search\",\n",
    "    \"Machine learning models require large datasets\",\n",
    "    \"Natural language processing transforms text data\",\n",
    "    \"Artificial intelligence revolutionizes technology\",\n",
    "    \"Data science combines statistics and programming\"\n",
    "]\n",
    "\n",
    "embeddings, embedding_stats, embedding_results = await analyze_embeddings(sample_texts)\n",
    "print(f\"\\n‚úÖ Generated embeddings with dimension {embedding_stats['dimension']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Embedding norms\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "axes[0, 0].bar(range(len(norms)), norms)\n",
    "axes[0, 0].set_title('Embedding Vector Norms')\n",
    "axes[0, 0].set_xlabel('Text Index')\n",
    "axes[0, 0].set_ylabel('L2 Norm')\n",
    "\n",
    "# Embedding value distribution\n",
    "axes[0, 1].hist(embeddings.flatten(), bins=50, alpha=0.7)\n",
    "axes[0, 1].set_title('Embedding Value Distribution')\n",
    "axes[0, 1].set_xlabel('Embedding Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Similarity matrix\n",
    "similarity_matrix = np.dot(embeddings, embeddings.T)\n",
    "im = axes[1, 0].imshow(similarity_matrix, cmap='viridis')\n",
    "axes[1, 0].set_title('Cosine Similarity Matrix')\n",
    "axes[1, 0].set_xlabel('Text Index')\n",
    "axes[1, 0].set_ylabel('Text Index')\n",
    "plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "# Processing time vs token count\n",
    "token_counts = [result.token_count for result in embedding_results]\n",
    "processing_times = [result.processing_time for result in embedding_results]\n",
    "axes[1, 1].scatter(token_counts, processing_times)\n",
    "axes[1, 1].set_title('Processing Time vs Token Count')\n",
    "axes[1, 1].set_xlabel('Token Count')\n",
    "axes[1, 1].set_ylabel('Processing Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display embedding statistics\n",
    "print(\"\\nüìä Embedding Statistics:\")\n",
    "for key, value in embedding_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate system recommendations based on analysis\n",
    "def generate_recommendations(performance_df, response_df, embedding_stats):\n",
    "    \"\"\"Generate system optimization recommendations.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Performance recommendations\n",
    "    best_config = performance_df.loc[performance_df['avg_score'].idxmax()]\n",
    "    recommendations.append(\n",
    "        f\"üéØ Optimal search configuration: Vector weight {best_config['vector_weight']:.1f}, \"\n",
    "        f\"Keyword weight {best_config['keyword_weight']:.1f}\"\n",
    "    )\n",
    "    \n",
    "    avg_search_time = performance_df['search_time'].mean()\n",
    "    if avg_search_time > 1.0:\n",
    "        recommendations.append(\n",
    "            f\"‚ö° Consider optimizing search performance (current avg: {avg_search_time:.3f}s)\"\n",
    "        )\n",
    "    \n",
    "    # Response quality recommendations\n",
    "    low_confidence_count = (response_df['confidence_score'] < 0.7).sum()\n",
    "    if low_confidence_count > 0:\n",
    "        recommendations.append(\n",
    "            f\"üîç {low_confidence_count} responses have low confidence - consider improving source quality\"\n",
    "        )\n",
    "    \n",
    "    avg_source_coverage = response_df['source_coverage'].mean()\n",
    "    if avg_source_coverage < 0.6:\n",
    "        recommendations.append(\n",
    "            f\"üìö Low source coverage ({avg_source_coverage:.2f}) - consider expanding knowledge base\"\n",
    "        )\n",
    "    \n",
    "    # Embedding recommendations\n",
    "    if embedding_stats['std_norm'] > 0.1:\n",
    "        recommendations.append(\n",
    "            \"üìê High variance in embedding norms - consider normalization\"\n",
    "        )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate and display recommendations\n",
    "recommendations = generate_recommendations(performance_df, response_df, embedding_stats)\n",
    "\n",
    "print(\"\\nüéØ System Optimization Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "if not recommendations:\n",
    "    print(\"‚úÖ System is performing optimally based on current analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save performance analysis\n",
    "performance_df.to_csv(output_dir / \"search_performance_analysis.csv\", index=False)\n",
    "print(f\"üíæ Saved search performance analysis to {output_dir / 'search_performance_analysis.csv'}\")\n",
    "\n",
    "# Save response quality analysis\n",
    "response_df.to_csv(output_dir / \"response_quality_analysis.csv\", index=False)\n",
    "print(f\"üíæ Saved response quality analysis to {output_dir / 'response_quality_analysis.csv'}\")\n",
    "\n",
    "# Save embedding statistics\n",
    "with open(output_dir / \"embedding_statistics.json\", \"w\") as f:\n",
    "    json.dump(embedding_stats, f, indent=2)\n",
    "print(f\"üíæ Saved embedding statistics to {output_dir / 'embedding_statistics.json'}\")\n",
    "\n",
    "# Save recommendations\n",
    "with open(output_dir / \"system_recommendations.txt\", \"w\") as f:\n",
    "    f.write(\"QdrantRAG-Pro System Recommendations\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        f.write(f\"{i}. {rec}\\n\")\n",
    "print(f\"üíæ Saved recommendations to {output_dir / 'system_recommendations.txt'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete! All results exported to data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
